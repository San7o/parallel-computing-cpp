\iffalse

References draft:

Jaeyoung Choi, Jack J. Dongarra, David W. Walker.  Parallel Matrix Transpose Algorithms on Distributed Memory Concurrent Computers.  Mathematical Sciences Section Oak Ridge National Laboratory P.O. Box 2008, Bldg. 6012 Oak Ridge, TN 37831-6367
- Notes: The communication is based of the greatest commond divisor (GCD) of the number of rows and columns (P and Q). If P and Q are relatively prima, the matrix transpose algorithm involves complete exchange communication. A block is LCM/P x LCM/Q if not relativaly prime and only LCM/GCD communications are required.
- The algorithms make use of non-blocking, point-to-point communication between proces-
sors. The use of nonblocking communication allows a processor to overlap the messages that it sends to different processors, thereby avoiding unnecessary synchronization.

Jonathan Eckstein, Gyorgy Matyasfalvi. Efficient Distributed-Memory Parallel Matrix-Vector Multiplication with Wide or Tall Unstructured Sparse Matrices.
- on sparse matrixes, but Its about vector multiplications

John C. Bowman, Malcolm Roberts. Adaptive Matrix Transpose Algorithms for Distributed Multicore Processors.
- really nice for my problem
- hybrid approach with both shared and distributed
- Notes: "The most popular algorithms for transposing an NxN matrix distributed over P processes are the direct communicatoin (all-to-all, simpler implementation) and recursive binary exchange (butterfly) algorithms."

Jaeyoung Choi, Jack Dongarra, David W. Walker. The design of scalable software libraries for distributed memory concurrent computers. Oak Ridge National Laboartory Mathematical Sciences Section. Oak RIdge, TN 37831-6367.
- they explained (apparently) block cyclic data distribution. Actually no


So we learned that block cyclic data distribution (round robin) is efficient for data access. You can mix blocks with cycles to get a few combinations to try out.

Sheng Ma, Libo Huang, Mingche Lai, Wei Shi. Network-on-chip customizations for message passing interface primitives: From Implementations to Programming Paradigms, 2015
- In conventional MPI implementations, MPI_Bcast is typically implemented using software with a tree-based algorithm. Such implementations exploit point-to-point communication operations. Thus, the number of hops to reach leaf nodes increases with the total number of nodes (typically in a logarithmic manner); the latency of MPI_Bcast also increases.
- this is useful in the findings because we find the same outcome
- but I use scatter and gather... i should look at the implementation

From the source code: coll/coll_base_scatter:
- There are 3 implementations of scatter:
  - binomial tree algorithm, as discussed above uses point-to-point communications
  - linear functions which work the same for small values
  - linear non blocking
- The decision is taken in coll/tuned/coll_tuned_scatter_decision.c

\fi


\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{blindtext}  % generate random text
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{hyperref}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Matrix Transposition and Symmetry on Distributed Memory Systems\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
%should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Giovanni Santini}
\IEEEauthorblockA{\textit{Dept.\ of Information Engineering and Computer Science} \\
\textit{University of Trento}\\
Trento, Italy \\
giovanni.santini@studenti.unitn.it \\
MAT. 235441}
}

\maketitle

\begin{abstract}
TODO for later
\end{abstract}

\begin{IEEEkeywords}
parallel computing, matrix transpose, symmetric matrix, benchmark
\end{IEEEkeywords}

\section{Introduction}

Introduction here

In section 2, we discuss the state of the art of the two algorithms.
In section 3, we elaborate on the algorithms analyzed.
In section 4, we explain the benchmarking procedures and system information.
In section 5, we present the results. The conclusion is in section 6. Section 7 contains
the project's github link


\section{State of the Art}

TODO

\section{Contribution and Methodology}

TODO

\section{Experiments and System Description}

\iffalse
- Detailed description of the computing system and platform. \\
- Relevant specifications or configurations (e.g., libraries
and programming toolchains). \\
- Description of the experimental setup, procedures, and
methodologies used in the project. \\
- Discussion on how experiments are designed to test the hypotheses
or achieve the objectives \\
\fi

TODO

\section{Results and Discussion}

\iffalse
- Presentation of results \\
- Analysis and interpretation in context \\
- Comparison with the state-of-the-art \\
\fi

TODO

\section{Conclusions}

\iffalse
- Summary of the key findings and contributions
\fi 

TODO

\section{GIT and Instructions for Reproducibility}

Github link: \href{https://github.com/San7o/parallel-computing-cpp}{https://github.com/San7o/parallel-computing-cpp} \\
Please read the instructions in the \textit{README.md} for building.

\begin{thebibliography}{00}
\bibitem{b1} TODO
\end{thebibliography}
\vspace{12pt}

\end{document}
