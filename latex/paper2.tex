\iffalse

References draft:

Jaeyoung Choi, Jack J. Dongarra, David W. Walker.  Parallel Matrix Transpose Algorithms on Distributed Memory Concurrent Computers.  Mathematical Sciences Section Oak Ridge National Laboratory P.O. Box 2008, Bldg. 6012 Oak Ridge, TN 37831-6367
- Notes: The communication is based of the greatest commond divisor (GCD) of the number of rows and columns (P and Q). If P and Q are relatively prima, the matrix transpose algorithm involves complete exchange communication. A block is LCM/P x LCM/Q if not relativaly prime and only LCM/GCD communications are required.
- The algorithms make use of non-blocking, point-to-point communication between proces-
sors. The use of nonblocking communication allows a processor to overlap the messages that it sends to different processors, thereby avoiding unnecessary synchronization.

Jonathan Eckstein, Gyorgy Matyasfalvi. Efficient Distributed-Memory Parallel Matrix-Vector Multiplication with Wide or Tall Unstructured Sparse Matrices.
- on sparse matrixes, but Its about vector multiplications

John C. Bowman, Malcolm Roberts. Adaptive Matrix Transpose Algorithms for Distributed Multicore Processors.
- really nice for my problem
- hybrid approach with both shared and distributed
- Notes: "The most popular algorithms for transposing an NxN matrix distributed over P processes are the direct communicatoin (all-to-all, simpler implementation) and recursive binary exchange (butterfly) algorithms."

Jaeyoung Choi, Jack Dongarra, David W. Walker. The design of scalable software libraries for distributed memory concurrent computers. Oak Ridge National Laboartory Mathematical Sciences Section. Oak RIdge, TN 37831-6367.
- they explained (apparently) block cyclic data distribution. Actually no


So we learned that block cyclic data distribution (round robin) is efficient for data access. You can mix blocks with cycles to get a few combinations to try out.

Sheng Ma, Libo Huang, Mingche Lai, Wei Shi. Network-on-chip customizations for message passing interface primitives: From Implementations to Programming Paradigms, 2015
- In conventional MPI implementations, MPI_Bcast is typically implemented using software with a tree-based algorithm. Such implementations exploit point-to-point communication operations. Thus, the number of hops to reach leaf nodes increases with the total number of nodes (typically in a logarithmic manner); the latency of MPI_Bcast also increases.
- this is useful in the findings because we find the same outcome
- but I use scatter and gather... i should look at the implementation

From the source code: coll/coll_base_scatter:
- There are 3 implementations of scatter:
  - binomial tree algorithm, as discussed above uses point-to-point communications
  - linear functions which work the same for small values
  - linear non blocking
- The decision is taken in coll/tuned/coll_tuned_scatter_decision.c

GPU:
J. Gómez-Luna, I. -J. Sung, L. -W. Chang, J. M. González-Linares, N. Guil and W. -M. W. Hwu, "In-Place Matrix Transposition on GPUs," in IEEE Transactions on Parallel and Distributed Systems, vol. 27, no. 3, pp. 776-788, 1 March 2016, doi: 10.1109/TPDS.2015.2412549. 

Wonderful paper about cache:
S. Chatterjee and S. Sen, "Cache-efficient matrix transposition," Proceedings Sixth International Symposium on High-Performance Computer Architecture. HPCA-6 (Cat. No.PR00550), Touluse, France, 2000, pp. 195-205, doi: 10.1109/HPCA.2000.824350.

\fi


\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{blindtext}  % generate random text
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{hyperref}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Comparing shared and distributed memory performance: A case study on Matrix Transposition and Symmetry Check\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
%should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Giovanni Santini}
\IEEEauthorblockA{\textit{University of Trento}\\
Trento, Italy \\
giovanni.santini@studenti.unitn.it \\
MAT. 235441 \\
\href{https://github.com/San7o/parallel-computing-cpp}{github.com/San7o/parallel-computing-cpp}}
}

\maketitle

\begin{abstract}
TODO for later \end{abstract}

\begin{IEEEkeywords}
parallel computing, matrix transpose, symmetric matrix, benchmark
\end{IEEEkeywords}

\section{Introduction}

Shared-memory systems are widely used in performance-critial applications
to leverage multiple CPU cores on the same machine.
There are, however, limitations to this approach due to all CPU cores
competing for access to memory over a shared bus, which creates contention
and reduces performance. Additionally, scaling the number of cores in
shared-memory systems is challenging. Consequently, distributed-memory
models are widely used, as they allow memory operations to be
distributed across multiple computing nodes, and the system can
be scaled by adding new nodes to the cluster. While distributed-memory
systems offer significant advantages, they
depend on the network layer to exchange information, which can
introduce network latency and bandwidth saturation as potential
bottlenecks. To mitigate these challenges, technologies such as
InfiniBand \cite{b0}, RDMA \cite{b-1} and local caching \cite{b-2}
aim to minimize communication overhead and
enhance data transfer efficiency. Modern MPI implementations,
including OpenMPI and MPICH2, are optimized to utilize these
technologies \cite{b1} \cite{b2}, enabling improved performance in
distributed-memory environments. The two memory models each
have distinct strengths and limitations,
making a comparison between the two essential for evaluating optimal
system performance. In this report, we present an implementation of
matTranspose and checkSymm on distributed-memory systems using MPI.
This implementation is then compared with a shared-memory counterpart
developed using OpenMP. Additionally, we discuss various compiler
optimization strategies and explore implicit parallelism techniques,
such as SIMD optimizations.

The rest of the report is organized as follows.
In section 2, we discuss the state of the art of the two algorithms.
In section 3, we elaborate on the algorithms analyzed.
In section 4, we explain the benchmarking procedures and system
information. In section 5, we present the results. The conclusion
is in section 6. Section 7 contains the project's github link

\section{State of the Art}

\iffalse
- Parallel Matrix Transpose Algorithms on Distributed Memory Concurrent
  Computers
- Adaptive Matrix Transpose Algorithms for Distributed Multicore Processors
- Efficient Distributed-Memory Parallel Matrix-Vector Multiplication with
  Wide or Tall Unstructured Sparse Matrices.
- In-Place Matrix Transposition on GPUs

\fi

Distributed-memory matrix transposition is a well discussed problem.
Choi, Dongarra and Walker \cite{b3} presented an implementation
based on the block cyclic data distribution. In their approach, the
matrix is divided into blocks of arbitrary size. If the numbers of rows and
columns are relatively prime, then the algorithm consists of a two-dimension
complete exchange of information where each processor sends a sub block
to every other processor. Binary exchange was shown to be the faster communication
scheme \cite{b4} which completes the exchange in $log_2 P$ steps where
$P$ is the number of processors. If the number of rows and columns
are not relatively prime, processors are grouped into a number of groups
equal to the greatest common divisor of the number of rows and columns, and
communication is overlapped between groups using
non-blocking point-to-point communication. Bowman and Roberts \cite{b5} later
presented an hybrid approach leveraging both shared and distributed memory.
Eckstein and Matyasfalvi
\cite{b6} studied the distributed problem on wide or tall unstructured sparce matrices.
Their implementation assignes nonzero elements of the matrix evenly
between processes by partitioning the matrix by sections of non zero
elements of the same length. Juan Gómez-Luna et. al. \cite{b7} presented
several GPU applications and an heuristic to detect the optimal approach.

\section{Contribution and Methodology}

In this section, implementations of both \textit{matTranspose}
and \textit{checkSymm} using MPI are presented, starting with
\textit{matTranspose}. We benchmark the bandwidth and speedup
of different MPI implementations compared to a serial and to a OpenMPI one.
Different compiler optimizations will also be compared. All benchmarks
will be tested against every matrix size from $2^2$ to $2^{12}$,
we will assume the number of processes is divisible by the matrix size
and the square root is an integer number for ease of implementation,
however the algorithms can be generalised with few modifications.
The algorithms for \textit{matTranspose} will now be explained.

\textit{matTranspose} takes a $N \times N$ input square matrix
\textit{input}, a matrix of the same shape \textit{ouput}, and the
size of the two matrices \textit{size}. The analyzed algorithms are the following:
\begin{itemize}
\item \textit{matTranspose}: naive implementation of matTranspose, iterating over values of the input matrix and storing them in the output matrix with indexes of rows and columns swapped.
\item \textit{matTransposeMPI}: this implementation takes full advantage
  of MPI datatypes to test bandwidth. MPI has the possibility to define a
  custom datatype to access memory, we used \textit{MPI\_Type\_contiguous}
  to define a row type and \textit{MPI\_Type\_vector} + \textit{MPI\_Type\_create\_resized}
  for a column type. The root process scatters rows with \textit{MPI\_Scatter} and
  retrieves columns with \textit{MPI\_Gather}, therefore
  transposing the matrix. Each process receives a block of rows and
  sends back the same block to the root but as column type. The algorithm
  automatically assign the correct number of rows/column to each process based on the
  number of processes so that the entire matrix is transposed.
  For example, in a $16x16$ matrix with $4$ processes, each processes
  receives $\frac{16}{4}=4$ rows and sends the same number back as
  columns.
  This implementation fully tests the communication
  bandwidth and latency by relying only on communication and MPI
  functionalities.
\item \textit{matTransposeMPIBlock}: this implementation divides the
  original matrix into a mesh of square sub-matrixes with side length equal to
  $\frac{N}{\sqrt{\#processes}}$ using MPI datatypes. Each sub-matrix is assigned
  to a different processes. The root process sends the sub-matrix to the
  other processes with \textit{MPI\_Scatterv} and receives the same
  number of sub-matrices but in transposed order with \textit{MPI\_Gatherv},
  for example process[0][1] will sand to root[1][0].
  Each process tranposes its sub-matrix and sends It back to the root.
  This approach tests a different communication method.
\end{itemize}
The algorithm for \textit{checkSymm} will now be explained. The algorithm
takes an $N \times N$ square matrix \textit{input}, a size \textit{size}
and returns true if the matrix is symmetrix, false otherwise.
The algorithm the following:
\begin{itemize}
\item \textit{checkSymm}: naive implementation of checkSymm. It Iterates over the upper half of the matrix and testing if every value is equal to the symmetric value.
\item \textit{checkSymmMPI}: similar to \textit{matTransposeMPIBlock},
but each process receives both a sub-matrix and the transposed sub-matrix,
that is, if we treated each sub-matrix as a single element of a matrix,
the element in the tranposed position will be received. Each process compares the two
sub-matrices to check if they are symmetric. The result is computed
via \textit{MPI\_Reduce}.
\end{itemize}

\section{Experiments and System Description}

\iffalse
- Detailed description of the computing system and platform. \\
- Relevant specifications or configurations (e.g., libraries
and programming toolchains). \\
- Description of the experimental setup, procedures, and
methodologies used in the project. \\
- Discussion on how experiments are designed to test the hypotheses
or achieve the objectives \\
\fi

Experiments were conducted on a cluster with 96 Intel(R) Xeon(R) Gold
6252N CPUs (2.30GHz) featuring 32K L1d and L1i caches, 1022K L2 cache,
36608K L3 cache (64-bit cache lines), 1 thread per core, and 4GB RAM,
running Linux 3.10. The algorithms were benchmarked using \textit{valFuzz},
with benchmarks compiled using gcc-9.1.0. Four benchmark sets were
compiled with different optimization levels: none, -O1, -O2 and "-O3
-Ofast -march=native".

To ensure statistical significance, each benchmark was executed 100
times, recording wall-clock execution time statistics: minimum, maximum,
median, mean, standard deviation, Q1, and Q3. Cache interference was
minimized by clearing it before each run by writing to an array twice
the L3 cache size. Input matrices, consistent across algorithms, were
initialized with uniformly random floats (-1 to 1) generated via a
Linear Congruential Generator (LCG) \cite{b8} with parameters a=1664525,
c=1013904223, m=232 \cite{b9}, and seed 1337.

To expedite random number generation, values were precomputed at
compile time using \textit{tenno-tl}. Additional \textit{tenno-tl}
features, such as ranges and arrays, were also employed.

\section{Results and Discussion}

\iffalse
- Presentation of results \\
- Analysis and interpretation in context \\
- Comparison with the state-of-the-art \\
\fi

TODO

\section{Conclusions}

\iffalse
- Summary of the key findings and contributions
\fi 

TODO

\section{GIT and Instructions for Reproducibility}

Github link: \href{https://github.com/San7o/parallel-computing-cpp}{https://github.com/San7o/parallel-computing-cpp} \\
Please read the instructions in the \textit{README.md} for building.

\begin{thebibliography}{00}
\bibitem{b0} P. Grun, Introduction to InfiniBand for End Users, InfiniBand
Trade Assoc., Beaverton, OR, USA, 2010. [Online]. Available:
cw.infinibandta.org/document/dl/7268
\bibitem{b-1} Kalia, A., Kaminsky, M., and Andersen, D. G. (2016). Design Guidelines for High Performance RDMA Systems. 2016 USENIX Annual Technical Conference (USENIX ATC 16), 437–450. https://www.usenix.org/conference/atc16/technical-sessions/presentation/kalia 
\bibitem{b-2} Message Passing Interface Forum. (2023). MPI: A Message-Passing Interface Standard Version 4.1. Chapter 8.7 "Caching". https://www.mpi-forum.org/docs/mpi-4.1/mpi41-report.pdf 
\bibitem{b1} G. M. Shipman, T. S. Woodall, R. L. Graham, A. B. Maccabe and P. G. Bridges, "Infiniband scalability in Open MPI," Proceedings 20th IEEE International Parallel and Distributed Processing Symposium, Rhodes, Greece, 2006, pp. 10 pp.-, doi: 10.1109/IPDPS.2006.1639335.
\bibitem{b2} J. Liu et al., "Design and implementation of MPICH2 over InfiniBand with RDMA support," 18th International Parallel and Distributed Processing Symposium, 2004. Proceedings., Santa Fe, NM, USA, 2004, pp. 16-, doi: 10.1109/IPDPS.2004.1302922.
\bibitem{b3} Jaeyoung Choi, Jack J. Dongarra, David W. Walker.  Parallel Matrix Transpose Algorithms on Distributed Memory Concurrent Computers.  Mathematical Sciences Section Oak Ridge National Laboratory P.O. Box 2008, Bldg. 6012 Oak Ridge, TN 37831-6367
\bibitem{b4} S. Takkella and S. Seidel, "Complete exchange and broadcast algorithms for meshes," Proceedings of IEEE Scalable High Performance Computing Conference, Knoxville, TN, USA, 1994, pp. 422-428, doi: 10.1109/SHPCC.1994.296674.
\bibitem{b5} John C. Bowman and Malcolm Roberts. Adaptive Matrix Transpose Algorithms for Distributed Multicore Processors
\bibitem{b6} Eckstein, J., Matyasfalvi, G. (2018). Efficient Distributed-Memory Parallel Matrix-Vector Multiplication with Wide or Tall Unstructured Sparse Matrices. https://arxiv.org/abs/1812.00904 
\bibitem{b7} J. Gómez-Luna, I. -J. Sung, L. -W. Chang, J. M. González-Linares, N. Guil and W. -M. W. Hwu, "In-Place Matrix Transposition on GPUs," in IEEE Transactions on Parallel and Distributed Systems, vol. 27, no. 3, pp. 776-788, 1 March 2016, doi: 10.1109/TPDS.2015.2412549.
\bibitem{b8} Rotenberg, A. (1960). "A New Pseudo-Random Number Generator". Journal of the ACM. 7 (1): 75–77
\bibitem{b9} Numerical Recipes ranqd1, Chapter 7.1, An Even Quicker Generator, Eq. 7.1.6
parameters from Knuth and H. W. Lewis
\end{thebibliography}
\vspace{12pt}

\end{document}
