\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{blindtext}  % generate random text
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[noend]{algpseudocode}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Matrix Transposition and Symmetry: a case study on Superscalar Architectures\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
%should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Giovanni Santini}
\IEEEauthorblockA{\textit{Dept.\ of Information Engineering and Computer Science} \\
\textit{University of Trento}\\
Trento, Italy \\
giovanni.santini@unitn.it \\
MAT. 235441}
}

\maketitle

\begin{abstract}
TODO *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
parallel computing, matrix transpose, symmetrix matrix. benchmarking
\end{IEEEkeywords}

\section{Introduction}

Modern CPUs use superscalar architectures to improve performance, combining multiple cores with advanced Single Instruction Multiple Data (SIMD) technologies. Examples include Intel's Streaming SIMD Extensions (SSE) and Advanced Vector Extensions (AVX). Although SSE is not a new technology, being first introduced in the Pentium III in 1999 and subsequently expanded in later years \cite{b1}, modern algorithms leveraging SIMD instructions continue to be a topic of active research in recent literature.
For example, the FreeBSD's SIMD-enhanced libc is actively reworking the standard library's code. \cite{b2}.
In 2023, most of the string functions were reimplemented by Robert Clausecker et al on many architectures \cite{b3}, exploiting vector instructions and improving the overall performance of the standard library \cite{b4}.
Similarly, efficient multi-threaded algorithms are becoming more critical as the demand of computation increases worldwide due to AI training \cite{b5} \cite{b6}.
This case study examines the implementations and performance of two algorithms—out-of-place square matrix transposition (matTranspose) and square matrix symmetry checking (checkSymm)—in the context of a superscalar architecture. Both algorithms have been used pervasively in performance critical applications. For example, matrix transposition was used in NASA's Apollo 11 Guidance Computer (AGC) to convert vectors in platenary coordinate system to base reference system \cite{b7}. Additionally, matrix transposition is used by Fourier Transform in the West (FFTW) \cite{b8}, a popular library to calculate Fast Furier Transofrms, and in signal processing \cite{b9}. Furthermore, the current C++26 draft proposes basic linear algebra algorithms in the "linalg" header, including matrix transposition \cite{b10}.

In section 2, we discuss the state of the art of the two algorithms.
In section 3, we elaborate on the algorithms analyzed, discuss performance evaluation and we present a new benchmarking framework.
In section 4, we show experimental data and graphs that is analyzed
in section 5. The conclusion is in section 6.


\section{State of the Art}

\textit{We define the transpose of a matrix $A$, and we denote it by $A^t$, as the matrix obtained from $A$ by interchanging rows and columns. Specifically, if $A$ is an $m$-by-$n$ matrix, then $A^t$ is the $n$-by-$m$ matrix whose entries are given by the equation (1)}. \cite{b11}


\begin{equation}
	(A^t)_{k,j} = A_{j,k}
\end{equation}

Computing the transpose of a matrix is a well discussed computational problem. Early
work focuses on in-situ transporision, that is transposing a matrix without
relying on additional space. Implementations were often based on the cyclic
structure of the transposition mapping: values of a matrix of size $N$ can be stored
in a single contiguous array or size $N$*$N$ and indexed with $row*\#rows+ column$.
Remarcable contributions include Boothroyd's ACM Algorithm 302 (1966) using cyclic mapping \cite{b13}, Laflin and Brefner ACM Algorithm 380
adding fixed points arithmetic \cite{b14} and
a later revision of the two by Esko G. Cate and David W. Twigg in ACM Algorithm 513 \cite{b15} .
SIMD implementations for specific architectures are also widely adopted
in performance critical applications like cryptographic libraries using instructions like Intel's vpunpckldq and vpunpckhdq \cite{b16} reporting improvements up to 70\% \cite{b17}.
However, matrix operations are not efficient as vector registers access data only linearly, motivating research on new vector register file (VFS) designs such as diagonal registers \cite{b23}.
Another area of research includes out-of-core matrix transposition, that
is computing the transpose of a matrix that exceeds the size of the available in-core memory.
This problem focuses on optimizing the number of I/O operations and file layout since memory-memory data transfer is notably slower than math operations. Work in out-of-core transposition include W. 0. Alltop (1975) \cite{b18}, Suh \& Prasanna (2002) \cite{b19} and Krishnamoorth et al. (2004) \cite{b20}.
Recent work explores parallelization and the use of GPUs to offload algebra operations. Examples are I-Jui Sung et al \cite{b21} and Mark Harris \cite{b22}.


\textit{A square matrix $A$ is called symmetric if it equals its transpose} \cite{b12}.

While symmetric matrces are actively used in various areas of research as their
properties allow for several optimizations in various problems, an in depth case study of symmetry
checking on modern architectures is yet to be published.

\section{Contribution and Methodology}

\iffalse % pseudocode
  \begin{algorithm}
    \caption{My Algorithm}\label{euclid}
    \begin{algoritmic}[1]
      \procedure{MyProcedure}{}
      % use \state and \textit{} for variables
      % use \text for text
      % use \gets for assignments
      % use \If \EndIf \Return \Else
      \EndProcedure
    \end{algoritmic}
  \end{algorithm}
\endif 

- Elaborate on the unique contributions of your project \\
- Describe the methodology proposed, including algorithms (pseudo-code),
data structures, and parallelization techniques. \\
- Discuss the challenges faced and how they were addressed \\

In this report we benchmark different implementations of matTranspose
and checkSymm, comparing implicit and explicit parallelism
techniques. In particular, the benchmarks aim to increase cache and SIMD performance,
speedup with multiple threads and the impact of compile flags optimizations. Data will be compared through tables
and visually via graphs such as the roofline model. All benchmarks
will be tested against every matrix sixe from $2^2$ to $2^{12}$, we will assume the size is a multiple of 4 for ease of implementation.
We will now explain the methodology for matTranspose.
The algorithms being benchmarked take a $N \times N$ input square matrix, an output matrix with the same shape and the size of the matrix.
The following will be benchmarked:
\begin{itemize}
\item \textit{matTranspose}: naive implementation of matTranspose, iterating over values of the input matrix and storing them in the output matrix with rows and columns indexes swapped.

  \begin{algorithm}
    \caption{matTranspose}\label{matTranspose}
    \begin{algoritmic}[1]
      \procedure{matTranspose}{$Matrix input, Matrix output, int size$}
        \For{\textit{r} \textbf{ in } \textit{0..size}}
            \State \For{\textit{c} \textbf{ in } \textit{0..size}}
            \State $output[c][r] = input[r][c]$
            \EndFor
        \EndFor
      \EndProcedure
    \end{algoritmic}
  \end{algorithm}

    \item \textit{matTransposeColumns}: similar to \textit{matTranspose}, but iterating on the input matrix first by columns and then by rows.
  The purpose of this implementation is to test a different access pattern.

  \begin{algorithm}
    \caption{matTranspose columns}\label{matTranspose}
    \begin{algoritmic}[1]
      \procedure{matTransposeColumns}{$Matrix input, Matrix output, int size$}
        \For{\textit{r} \textbf{ in } \textit{0..size}}
            \State \For{\textit{c} \textbf{ in } \textit{0..size}}
            \State $output[r][c] = input[c][r]$
            \EndFor
        \EndFor
      \EndProcedure
    \end{algoritmic}
  \end{algorithm}

\item \item{matTransposeHalf}: similar to \textit{matTranspose}, but the algorithm
  iterates only on the upper half of the input matrix. For each
  pair $(i,j)$ of indices for rows and columns in the upper half of the input matrix $M$, the output matrix O will
  store store $O[i][j]=M[j][i]$ and $O[j][i] = M[i][j]$.

  \begin{algorithm}
    \caption{matTranspose half}\label{matTranspose}
    \begin{algoritmic}[1]
      \procedure{matTransposeHalf}{$Matrix input, Matrix output, int size$}
        \For{\textit{r} \textbf{ in } \textit{0..size}}
            \State \For{\textit{c} \textbf{ in } \textit{r..size}}
            \State $output[r][c] = input[c][r]$
            \State $output[c][r] = input[r][c]$
            \EndFor
        \EndFor
      \EndProcedure
    \end{algoritmic}
  \end{algorithm}
  
\item \item{matTransposeCyclic}: implementation of \cite{b13}, storing the matrix as a contiguous one-dimensional array of size $N*N$.
  
  \begin{algorithm}
    \caption{matTranspose cyclic}\label{matTranspose}
    \begin{algoritmic}[1]
      \procedure{matTransposeCyclic}{$Matrix input, Matrix output, int size$}
        \For{\textit{n} \textbf{ in } \textit{0..(size*size)}}
        \State int \textit{i} = $\frac{n}{size}$
        \State int \textit{j} = $n \mod N$
        $T[n] = M[size * j + i]$
        \EndFor
      \EndProcedure
    \end{algoritmic}
  \end{algorithm}

\item \textit{matTransposeIntrinsic}: based on \textit{matrix\_transpose4x4} from the linux kernel \cite{b16}, revised to work for out-of-place float matrices on x86 architectures and implemented using intrinsics. \textit{transpose\_4x4\_f32\_intrinsic} transposes a 4 by 4 matrix using vector registers of 128 bits by doing the following steps: load 4 rows of 4 floats from the the input matrix, combine pairs of rows in an alternating pattern (interleaving), shuffle to get the transpose and store them in the output matrix. \textit{matTransposeIntrinsic} iterates over 4x4 blockw in the input matrix and saves the computed transposed block in the output matrix with rows and columns swapped.

  \begin{algorithm}
    \caption{matTranspose intrinsic}\label{matTranspose}
    \begin{algoritmic}[1]
      \procedure{transpose\_4x4\_f32\_intrinsic}{float* src0, float* src1, float* src2, float* src3, float* dst0, float* dst1, float *dst2, float* dst3}
    \textbf{comment} Load rows  
    \_\_m128 row0 = \_mm\_loadu\_ps(src0);
    \_\_m128 row1 = \_mm\_loadu\_ps(src1);
    \_\_m128 row2 = \_mm\_loadu\_ps(src2);
    \_\_m128 row3 = \_mm\_loadu\_ps(src3);

    \textbf{comment} unpack and interleave rows
    \_\_m128 t0 = \_mm\_unpacklo\_ps(row0, row1);
    \_\_m128 t1 = \_mm\_unpackhi\_ps(row0, row1);
    \_\_m128 t2 = \_mm\_unpacklo\_ps(row2, row3);
    \_\_m128 t3 = \_mm\_unpackhi\_ps(row2, row3);

    \textbf{comment} Shuffle to form transposed rows
    \_\_m128 d0 = \_mm\_movelh\_ps(t0, t2);
    \_\_m128 d1 = \_mm\_movehl\_ps(t2, t0);
    \_\_m128 d2 = \_mm\_movelh\_ps(t1, t3);
    \_\_m128 d3 = \_mm\_movehl\_ps(t3, t1);

    \textbf{comment} Store transposed rows
    \_mm\_storeu\_ps(dst0, d0);
    \_mm\_storeu\_ps(dst1, d1);
    \_mm\_storeu\_ps(dst2, d2);
    \_mm\_storeu\_ps(dst3, d3);

      \EndProcedure
      \procedure{matTransposeIntrinsic}{$Matrix input, Matrix output, int size$}
        \For{int i = 0, i < size, i += 4}
        \For{int j = 0, j < size, j += 4}
        \State transpose\_4x4\_f32\_intrinsic(input[i][j],
        input[i+1][j], input[i+2][j], input[i+3][j], output[j][i], output[j+1][i], output[j+2][i], output[j+3][i])
        \EndFor
      \EndProcedure
    \end{algoritmic}
  \end{algorithm}

\item \textit{matTransposeIntrinsicCyclic}: same as \textit{matTransposeIntrinsic} but the matrix is saved as a one dimensional array similarly to \textit{matTransposeCyclic}
%\item matTranspose vectorization:
%\item matTranspose half vectorization:
\item \textit{matTransposeUnrollingOuter}: similar to \textit{matTranspose} but the outer loop is manually unrolled

  \begin{algorithm}
    \caption{matTransposeUnrollingOuter}\label{matTranspose}
    \begin{algoritmic}[1]
      \procedure{matTransposeUnrollingOuter}{$Matrix input, Matrix output, int size$}
        \For{$int c = 0, i < size, i += 4$}
            \State \For{\textit{r} \textbf{ in } \textit{0..size}}
            \State $output[c][r] = input[r][c]$
            \State $output[c+1][r] = input[r][c+1]$
            \State $output[c+2][r] = input[r][c+2]$
            \State $output[c+3][r] = input[r][c+3]$
            \EndFor
        \EndFor
      \EndProcedure
    \end{algoritmic}
  \end{algorithm}

\item \textit{matTransposeUnrollingInner}: same as \textit{matTransposeUnrollingOuter} but the inner for loop is manually unrolled

\item \textit{matTransposeHalfUnrollingOuter}: same as \textit{matTransposeHalf} but the outer for loop is manually unrolled

\item \textit{matTransposeHalfUnrollingInner}: same as \textit{matTransposeHalf} but the inner for loop is manually unrolled

\item \textit{matTransposeCyclicUnrolled}: same as \textit{matTransposeCyclic} but the for loop in manually unrolled

  \begin{algorithm}
    \caption{matTranspose cyclic unrolled}\label{matTranspose}
    \begin{algoritmic}[1]
      \procedure{matTransposeCyclicUnrolled}{$Matrix input, Matrix output, int size$}
        \For{\textit{n} \textbf{ in } \textit{0..(size*size)}}
        $T[n] = M[size * ((n+1) \mod size ) + (\frac{n}{size})]$
        $T[n+1] = M[size * ((n+1+1) \mod size ) + (\frac{n+1}{size})]$
        $T[n+2] = M[size * ((n+2+1) \mod size ) + (\frac{n+2}{size})]$
        $T[n+3] = M[size * ((n+3+1) \mod size ) + (\frac{n+3}{size})]$
        \EndFor
      \EndProcedure
    \end{algoritmic}
  \end{algorithm}
  
\item \textit{matTransposeOmpX}: this set of algorithms are all derived from \textit{matTranspose} with the outer for loops parallelized using omp with the number of threads \textit{X} being a power of 2 from $2^1$ to $2^6$.

\item \textit{matTransposeOmpXCollapse}: similar to \textit{matTransposeOmpX} but the for loops are parellelized using \textit{collapse(2)}
  
\item \textit{matTransposeOmp16SchedX}: similar to \textit{matTransposeOmpX} with 16 threads using different schedule algorithms, namely: static, synamic, guided.
\end{itemize}

We will now explain the methodology for \textit{checkSymm}. The algorithms take an $N \times N$ square matrix and Its size are return true if the matrix is symmetrix, false otherwise.
The algorithms being benchmarked are the wollowing:
\being{itemize}
\item \textit{checkSymm}: naive implementation of checkSymm. Iterates over the upper half of the matrix and testing if every value is equal to the symmetric value.

  \begin{algorithm}
    \caption{checkSymm}\label{matTranspose}
    \begin{algoritmic}[1]
      \procedure{checkSymm}{$Matrix input, int size$}
        \State $isSymm = true$
        \For{\textit{i} \textbf{ in } \textit{0..size}}
            \For{\textit{j} \textbf{ in } \textit{i..size}}
            \If $input[i][j] != input[j][i]$
            \State $isSymm = false$
        \EndFor
        \Return $isSymm$
      \EndProcedure
    \end{algoritmic}
  \end{algorithm}

% \item \textit{checkSymColumns}:
  
  % \item \textit{checkSymVectorization}
\item \textit{checkSymmUnrolledOuter}: similar to \textit{checkSymm} but the outer loop is manually unrolled.

  \begin{algorithm}
    \caption{checkSymmUnrolledOuter}\label{matTranspose}
    \begin{algoritmic}[1]
      \procedure{checkSymmUnrolledOuter}{$Matrix input, int size$}
        \State $isSymm = true$
        \For{\textit{i} \textbf{ in } \textit{0..size}}
            \For{\textit{j} \textbf{ in } \textit{i..size}}
            \If $input[i][j] != input[j][i]$
            \State $isSymm = false$
            \If $input[i+1][j] != input[j][i+1]$
            \State $isSymm = false$
            \If $input[i+2][j] != input[j][i+2]$
            \State $isSymm = false$
            \If $input[i+3][j] != input[j][i+3]$
            \State $isSymm = false$
        \EndFor
        \Return $isSymm$
      \EndProcedure
    \end{algoritmic}
  \end{algorithm}

  
\item \textit{checkSymmUnrollingInner}: similar to \textit{checkSymmUnrolledOuter} but the inner loop is manually unrolled instead of the outer loop.
\item \textit{checkSymmUnrollingOuterNoBranch}:
\item \textit{checkSymmUnrollingInnerNoBranch}:
\item \textit{checkSymOmpX}:
\item \textit{checkSymOmpXCollapse}:
\item \textit{checkSymm16SchedX}:
\end{itemize}

\iffalse
Remember: write
- what you did (contribution): high level overview like "hey, i want to test different vectorization ond parallelization implementations"
- how you did it (methodology): we benchmarked algorithm A ... (all the pseudocodes). Whay we mesured

\fi

Quiesti qui bellissimi e stra fighi
- uno (pseudocodice)
- due (pseudocodice)
- tre (pseudocodice)
- ...
Ho usato implicit ed esplicit parallelism con vectorization e openmp
Voglio fare questo per comparare quest'altro cos' pel meme
Benchmarkare è difficle perchè... quindi ho fatto ciò... (aplia qua)


\section{Experiments and System Description}

- Detailed description of the computing system and platform. \\
- Relevant specifications or configurations (e.g., libraries
and programming toolchains). \\
- Description of the experimental setup, procedures, and
methodologies used in the project. \\
- Discussion on how experiments are designed to test the hypotheses
or achieve the objectives \\


All relevant cluster info, compiler, flags, c++ standard. Test flags and data. Benchmarks metrics collected
Example:
System Description:
The experiments were conducted on a system equipped with an Intel Core i9-12900K CPU (16 cores, 24 threads, base clock 3.2 GHz), 32 GB of DDR5 RAM, and an NVIDIA RTX 3080 GPU (10 GB GDDR6X). The operating system was Ubuntu 22.04, and all code was compiled using GCC 12.2 with the -O3 optimization flag. Benchmarks were run in an isolated environment with minimal background processes to ensure consistency.

Experimental Setup:
The study benchmarked three sorting algorithms: QuickSort, MergeSort, and HeapSort. Input datasets ranged from 1,000 to 1,000,000 elements and included random, sorted, and reverse-sorted orders. Each experiment was repeated 10 times, with execution time and memory usage recorded. Performance metrics were averaged across runs to account for variability.

Mamma mia che bel framework che ho

Ho runanto tutto sul pbs con questi cores bellissimi con le ottimizzazioni diverse. Ho collezionato i dativi in un basino e l'ho grafato con quella mmerda di pitone. Ho fatto i test così perchè mi piacevano.

\section{Results and Discussion}

- Presentation of results \\
- Analysis and interpretation in context \\
- Comparison with the state-of-the-art \\

cinquantamila grafici qui

Mamma mia ma quanto e' veloce sto asesmbly,
buu al compilatore che non ci azzecca per bene. 
Anche ciclico è nammerda, invece l'unrolling
è figo.
In generale abbiamo capito
cose ed il cluster è pazzesco e le differenze
sono notevoli, evviva i soldi.


\section{Conclusions}

- Summary of the key findings and contributions

Ciao ciao

\begin{thebibliography}{00}
\bibitem{b1} Intel® 64 and IA-32 Architectures Software Developer’s Manual Combined Volumes: 1, 2A, 2B, 2C, 2D, 3A, 3B, 3C, 3D, and 4. Volume 1, Section 2.2.7 "SIMD Instructions"
\bibitem{b2} SIMD man page "https://man.freebsd.org/cgi/man.cgi?simd(7)"
\bibitem{b3} Git blame on freeBSD's source code, in "lib/libc/amd64/string", shows recent activity
\bibitem{b4} https://freebsdfoundation.org/blog/a-sneak-peek-simd-enhanced-string-functions-for-amd64/
\bibitem{b5} Amodei, D. Hernandez, D. AI and Compute, https://openai.com/blog/ai-and-compute
\bibitem{b6} Andrew J. Lohn, Micah Musse. AI and Compute How Much Longer Can Computing Power Drive Artificial Intelligence Progress? CSET Issue Brief
\bibitem{b7} Digitalized Apollo 11 guidance computer by Virtual AGC and MIT Museum. Code in "Apollo-11/Comanche055/PLANETERY\_INTERNAL\_ORIENTATION.agc" line 34; "Apollo-11/Luminary099/ATTITUDE\_MANEUVER\_ROUTINE.agc" line 441
\bibitem{b8} Matteo Frigo and Steven G. Johnson. "The design and implementation of FFTW3". The word "transpose" is mentioned 21 times
\bibitem{b9} M. Henriksson and O. Gustafsson, "Streaming Matrix Transposition on FPGAs Using Distributed Memories," 2023 IEEE Nordic Circuits and Systems Conference (NorCAS), Aalborg, Denmark, 2023, pp. 1-6, doi: 10.1109/NorCAS58970.2023.10305472.
\bibitem{b10} "https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2023/p1673r12.html" Accessed in november 2024
\bibitem{b11} Sheldon Axler. Linear Algebra Done Right ourth edition
16 November 2024. Section 3C Matrices, definition 3.54
\bibitem{b12} Sheldon Axler. Linear Algebra Done Right ourth edition
16 November 2024. Section 9A Bilinear Forms and Quadratic Forms, definition 9.11
\bibitem{b13} BOOTHROYD,Z. Algorithm 302: Transpose vector stored array. Go~r~m.A~M I0, ~ (1967),
292-293
\bibitem{b14} LAFLIN, S., AND BREFNER, M.A. Algorithm 380: In-situ transposition of a rectangular
matrix, Comm. ACM IS, 5 (1970), 324-326
\bibitem{b15} ESKO G. CATE and DAVID W. TWIGG ALGORITHM 513: Analysis of In-Situ Transposition (1977)
\bibitem{b16} most of the crypto functions are written in assembly usin SIMD instructions in the
linux kernel "linux/arch/x86/crypto" for exanple transpose\_4x4 in "serpent-avx2-asm\_64.S", "sm4-aesni-avx-asm\_64.S",
"cast6-avx-x86\_64-asm\_64.S" and others
\bibitem{b17} https://github.com/torvalds/linux/commit/d34a460092d857f1616e39eed7eac6f40cea2225
\bibitem{b18} W. O. Alltop, "A Computer Algorithm for Transposing Nonsquare Matrices," in IEEE Transactions on Computers, vol. C-24, no. 10, pp. 1038-1040, Oct. 1975, doi: 10.1109/T-C.1975.224124.
\bibitem{b19} Jinwoo Suh and V. K. Prasanna, "An efficient algorithm for out-of-core matrix transposition," in IEEE Transactions on Computers, vol. 51, no. 4, pp. 420-438, April 2002, doi: 10.1109/12.995452.
\bibitem{b20} Kandemir, M. \& Choudhary, A. \& Ramanujam, J. \& Arunachalam, Meenakshi. (2000). A unified framework for optimizing locality, parallelism, and communication in out-of-core computations. Parallel and Distributed Systems, IEEE Transactions on. 11. 648 - 668. 10.1109/71.877759. 
\bibitem{b21} I-Jui Sung, Juan G´omez-Luna, Josè Marìa Gonzàlez-Linares
Nicolàs Gui, Wen-Mei W. Hwu. In-Place Transposition of Rectangular Matrices on Accelerators
\bibitem{b22} Post: An Efficient Matrix Transpose in CUDA C/C++. Feb 18, 2013 By Mark Harris, https://developer.nvidia.com/blog/efficient-matrix-transpose-cuda-cc/
\bibitem{b23} Hanounik, Bedros. (2000). Diagonal Registers: Novel Vector Register File Design for High Performance and Multimedia Computing. 
\end{thebibliography}
\vspace{12pt}

\end{document}
